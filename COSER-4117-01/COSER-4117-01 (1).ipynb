{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c04e88e-1c30-42f7-a039-cbb0b5e74e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5830cfd-ea30-47f1-bafa-e4a11def03a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ad8f45b-67b8-440a-982b-7fb218877862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/joheras/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/joheras/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "from jiwer import wer #Para la métrica\n",
    "import re\n",
    "from statistics import mean, stdev\n",
    "import PyPDF2\n",
    "\n",
    "import string \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import num2words\n",
    "from autocorrect import Speller #Para los errores ortográficos\n",
    "spell = Speller(lang='es')\n",
    "from jiwer import wer #Para la métrica\n",
    "import time # Para medir los tiempos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c0af54d-fbc1-4879-bdcf-57cd5f07df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para cargar el texto original si el audio es del COSER\n",
    "def originalCOREC(nombre):\n",
    "    with open('../COREC/'+nombre+'.pdf', 'rb') as file:\n",
    "        pdf = PyPDF2.PdfReader(file)\n",
    "    \n",
    "        texto = ''\n",
    "    \n",
    "        for pagina in range(len(pdf.pages)):\n",
    "            pagina_texto = pdf.pages[pagina].extract_text()\n",
    "            texto = texto + pagina_texto\n",
    "            \n",
    "    return texto\n",
    "\n",
    "def limpiarCOREC(original):\n",
    "    \n",
    "    original = original.replace('//', '. ').replace('/ ', ', ').replace('/', ', ') # Cambiamos las barras por puntos y comas\n",
    "    \n",
    "    original = re.sub(' \\[\\d\\d:\\d\\d\\]', '', original) # Quitamos las marcas de tiempo\n",
    "    original = re.sub('\\(\\d:\\d\\d\\)', '', original)\n",
    "    original = re.sub('\\(\\d\\d:\\d\\d\\)', '', original)\n",
    "    original = re.sub('\\[.*\\]{1}', '', original)\n",
    "    original = re.sub('\\(.*\\)', '', original)\n",
    "    original = re.sub('[(][^)]*[)]', '', original)\n",
    "    original = re.sub('[\\[][^\\]]*[\\]]', '', original)\n",
    "    \n",
    "    #original = original.replace('[', '').replace(']', '')\n",
    "\n",
    "    \n",
    "    encuestadores = ['E:', 'e:', 'E1:', 'E2:', 'E3:', 'E4:', 'E =', 'E=', 'E1 =', 'E1=', 'E2 =', 'E2=', 'E3 =', 'E3=', 'E4 =', 'E4=', 'E.1', 'E.2', 'E.3', 'E.4', 'E1.TL', 'E2.TL', 'E3.TL', 'E4.TL'] # Quitamos la marca del encuestador\n",
    "    for encuest in encuestadores:\n",
    "        original = original.replace(encuest, ' ')\n",
    "        \n",
    "    informantes = ['I:', 'I1:', 'I2:', 'I3:', 'I4:', 'NP:', 'NP', 'Mote: ', 'XXX', 'I =', 'I=', 'I1 =', 'I1=', 'I2 =', 'I2=', 'I3 =', 'I3=', 'I4 =', 'I4=', 'I.1', 'I.2', 'I.3', 'I.3', 'I1.TL', 'I2.TL', 'I3.TL', 'I4.TL'] # Quitamos la marca del informante\n",
    "    for inf in informantes:\n",
    "        original = original.replace(inf, ' ')\n",
    "        \n",
    "        \n",
    "    original = original.replace('/n', ' ') # Quitamos los saltos de línea\n",
    "    original = original.replace('\\n', ' ')       \n",
    "        \n",
    "    original = re.sub(' +', ' ', original)\n",
    " \n",
    "    return original\n",
    "\n",
    "def originalCOSER(nombre):\n",
    "    \n",
    "    \n",
    "    with open(nombre, 'r', encoding = 'ISO-8859-1') as f:\n",
    "        original = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    # Sacamos los datos del principio:\n",
    "    copia = []\n",
    "    copia.append(original)\n",
    "    \n",
    "    datos = {}\n",
    "    for c in range(len(copia)):\n",
    "        copia[c] = copia[c].split('\\n')  # Separamos los txt por párrafos\n",
    "        for i in range(len(copia[c])):\n",
    "            copia[c][i] = copia[c][i].split('\\t')\n",
    "    copia = copia[0]\n",
    "    \n",
    "    cont = 0\n",
    "    while copia[cont][0]!= 'Informantes:': # Nos guardamos la información hasta informantes\n",
    "        datos[copia[cont][0][:-1]] = copia[cont][1]\n",
    "        cont = cont + 1\n",
    "    datos[copia[cont][0][:-1]] = [copia[cont][1]]\n",
    "    a = cont\n",
    "    cont = cont +1\n",
    "    while 'Encuest' not in copia[cont][0]:\n",
    "        datos[copia[a][0][:-1]].append(copia[cont][0])\n",
    "        cont = cont+1\n",
    "        \n",
    "    while copia[cont][0] != 'Temas:':\n",
    "        datos[copia[cont][0][:-1]] = copia[cont][1]\n",
    "        cont = cont + 1\n",
    "    datos[copia[cont][0][:-1]] = []\n",
    "    datos[copia[cont][0][:-1]].append(copia[cont][1])\n",
    "    b = cont\n",
    "    cont = cont+1\n",
    "    while copia[cont]!=['']:\n",
    "        datos[copia[b][0][:-1]].append(copia[cont][0])\n",
    "        cont = cont+1\n",
    "        \n",
    "    cont = cont+1\n",
    "\n",
    "    texto = ''\n",
    "    \n",
    "    while copia[cont]!=['']:\n",
    "        texto = texto + copia[cont][0]\n",
    "        cont = cont + 1\n",
    "    #while copia[cont]!=['']:\n",
    "    #    if \"HS:\" in copia[cont][0]:\n",
    "    #        copia[cont][0] = 'CR, '+copia[cont][0]\n",
    "   #     datos[original[-1]].append(copia[cont])\n",
    "   #     cont = cont+1\n",
    "\n",
    "    #texto = ''\n",
    "    #for i in range(len(datos)):\n",
    "    #    texto = texto + datos[i][0]\n",
    "    #return datos, texto\n",
    "    return datos, texto\n",
    "\n",
    "\n",
    "\n",
    "def limpiarCOSER(original):\n",
    "    \n",
    "    original = re.sub(' \\[\\d\\d:\\d\\d\\]', '', original) # Quitamos las marcas de tiempo\n",
    "    \n",
    "    original = re.sub('\\|T\\d\\|', '', original) # Quitamos las marcas del tema que se trata\n",
    "    original = re.sub('\\|T\\d\\d\\|', '', original)\n",
    "    original = re.sub('\\|\\d\\|', '', original) \n",
    "    original = re.sub('[(][^)]*[)]', '', original)\n",
    "    original = re.sub('[\\[][^\\]]*[\\]]', '', original)\n",
    "    \n",
    "    encuestadores = ['E: ', 'E1: ', 'E2: ', 'E3: ', 'E4: '] # Quitamos la marca del encuestador\n",
    "    for encuest in encuestadores:\n",
    "        original = original.replace(encuest, ' ')\n",
    "        \n",
    "    informantes = ['I: ', 'I1: ', 'I2: ', 'I3: ', 'I4: ', 'NP: ', 'NP', 'Mote: ', 'XXX'] # Quitamos la marca del informante\n",
    "    for inf in informantes:\n",
    "        original = original.replace(inf, ' ')\n",
    "    \n",
    "    solapamientos = ['HS:E1', 'HS:E2', 'HS:E3', 'HS:E4', 'HS:E', 'HS:I1', 'HS:I2', 'HS:I3',\n",
    "                     'HS:I', 'HS:', 'HCruz:', 'HSim-O:', 'V-Sml', 'V-Ljn', 'V-Mrm', 'V-Otr', 'V-Tml']\n",
    "    for solap in solapamientos:\n",
    "        original = original.replace(solap, '')\n",
    "        \n",
    "    pronunciacion = ['P-Enf:', 'P-Rel:', 'P-Ssr:', 'P-Slb:', 'P-Otr:']\n",
    "    for pronu in pronunciacion:\n",
    "        original = original.replace(pronu, '')\n",
    "        \n",
    "    emisiones = ['Asent', 'RISAS', 'RISA', 'Rndo: ', 'Llndo: ', 'EXCL', 'TOS', 'CARRASP', 'CHASQ', 'ONOM',\n",
    "                 'RESPIR', 'OTRAS-EM']\n",
    "    for emi in emisiones:\n",
    "        original = original.replace(emi, ' ')\n",
    "        \n",
    "    pausas = ['PS', 'PS: ', 'SLNC']\n",
    "    for pausa in pausas:\n",
    "        original = original.replace(pausa, '')\n",
    "        \n",
    "    emisionesExternas = ['R-Inf', 'R-Vhc', 'R-Anm', 'R-Vcs', 'R-Cas', 'R-Prt', 'R-Glp', 'R-Mus', 'R-Cmp', \n",
    "                         'R-Vnt', 'R_Tlf', 'R-Ppl', 'R-Grb', 'R-Mcr', 'R-Ind']\n",
    "    for emis in emisionesExternas:\n",
    "        original = original.replace(emis, '')\n",
    "        \n",
    "    gestos = ['G-Mst', 'G-Imt', 'G-Gnr', 'G-Otr']\n",
    "    for gesto in gestos:\n",
    "        original = original.replace(gesto, '')\n",
    "        \n",
    "    inteligibilidad = ['A-Inn', 'A-PIn:', 'A-Nul', 'A-Pau', 'A-Crt', 'A-Err', 'A-InfErr', 'AT']\n",
    "    for intel in inteligibilidad:\n",
    "        original = original.replace(intel, '')\n",
    "        \n",
    "    literatura = ['LT-Rlt:', 'LT-Rct:', 'LT-Cnt:', 'LT-Rfn:', 'LT-Otr:', 'L-Otra:']\n",
    "    for lit in literatura:\n",
    "        original = original.replace(lit, '')\n",
    " \n",
    "    return original\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalizar(texto):\n",
    "    #print(texto)\n",
    "    tokens=word_tokenize(texto) #Separa el texto por palabras\n",
    "    #print(tokens)\n",
    "    texto = [w.lower() for w in tokens] #Pasamos las palabras a minúsculas\n",
    "    #print(texto)\n",
    "    texto = [texto[i].replace(' apdo.', ' apartado').replace(' art. ',' articulo ').replace(' atte.',' atentamente').replace(' avda.',' avenida').replace(' cap.',' capítulo').replace(' cía.',' compañía').replace(' coord.',' coordinador').replace(' d.',' don').replace(' dña.',' doña').replace(' dcho.',' derecho').replace(' dcha.',' derecha').replace(' depto.',' departamento').replace(' dr.',' doctor').replace(' dra.',' doctora').replace(' etc.',' etcétera').replace(' fdo.',' firmado').replace(' izqdo.',' izquierdo').replace(' izqda.',' izquierda').replace(' max.',' máximo').replace(' min.',' mínimo').replace(' núm.',' número').replace(' pág.',' página').replace(' ej.',' ejemplo').replace(' prov.',' provincia').replace(' sr.',' señor').replace(' sra.',' señora').replace(' srta.',' señorita').replace(' tfno.',' teléfono') for i in range(len(texto))] #quitamos las abreviaciones\n",
    "    for i in range(len(texto)):\n",
    "        if texto[i].isdigit() and (texto[i] not in string.punctuation):\n",
    "            texto[i]=num2words.num2words(texto[i], lang='es', ordinal=False)\n",
    "    tokens=[w.lower() for w in texto] #Pasamos las palabras a minúsculas por si había números\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    stripped = [re_punc.sub('',w) for w in tokens]\n",
    "    stripped=[stripped[i].replace('¿', '').replace('¡','').replace(\"'\",'') for i in range(len(tokens))] #quita los símbolos de puntuación que string.punctuation no tiene en cuenta\n",
    "    words = [word for word in stripped if word.isalpha()] #Elimina los signos de puntuación\n",
    "    return words\n",
    "\n",
    "# Para quedarnos con el texto normalizado y sin errores ortográficos\n",
    "def correcto(texto):\n",
    "    result = normalizar(texto)\n",
    "    for word in result:\n",
    "        word = spell(word)\n",
    "        \n",
    "    result_norm = \"\"\n",
    "    for i in range(len(result)):\n",
    "        result_norm = result_norm + ' ' + result[i]\n",
    "    result = result_norm\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d426b22a-7e4b-4ab1-b06b-7ed68df5db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos, original = originalCOSER('../COSER-4117-01.TXT') \n",
    "original = limpiarCOSER(original)\n",
    "original = correcto(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e7dbc9c-befa-440e-b461-38bca0fc87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../audios_large-v3/COSER-4117-01.txt') as f:\n",
    "    predicted = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c599b170-c901-4f09-b78b-704f715e74d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 16:40:04.871927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "wer = load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e71a3b2f-6aae-4b92-85de-cb7f248c2ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13326621783174983"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer.compute(predictions=[predicted],references=[original])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f49c39cb-4ecc-4f7a-98ec-ca0ee083da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import compute_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5de803a-ce91-45c8-8b78-147cc23c3ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = compute_measures([original], [predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f55975e-6f31-438c-9421-32a60e00ae6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(671, 1099, 348)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measures['deletions'],measures['substitutions'],measures['insertions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "260758a7-a58b-442e-9c3a-c34193bae316",
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted = []\n",
    "dele = ''\n",
    "for x in measures['ops'][0]:\n",
    "    if(x.type=='delete'):\n",
    "        deleted.append(str(measures['truth'][0][x.ref_start_idx:x.ref_end_idx]))\n",
    "for elem in deleted:\n",
    "    dele = dele + elem +'\\n'\n",
    "with open('delete-COSER-4117-01', 'a', encoding = 'utf-8') as f:\n",
    "    f.write(dele)\n",
    "f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72bc24e6-d8d0-4d65-acd5-9d2533c97106",
   "metadata": {},
   "outputs": [],
   "source": [
    "substituted = []\n",
    "subs = ''\n",
    "for x in measures['ops'][0]:\n",
    "    if(x.type=='substitute'):\n",
    "        substituted.append(str(measures['truth'][0][x.ref_start_idx:x.ref_end_idx]) + \" -->\" + str(measures['hypothesis'][0][x.hyp_start_idx:x.hyp_end_idx]))\n",
    "for elem in substituted:\n",
    "    subs = subs + elem + '\\n'\n",
    "with open('substitute-COSER-4117-01', 'a', encoding='utf-8') as f:\n",
    "    f.write(subs)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6928d789-f319-441d-b205-9709ec57facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inserted = []\n",
    "ins = ''\n",
    "for x in measures['ops'][0]:\n",
    "    if(x.type=='insert'):\n",
    "        inserted.append(measures['hypothesis'][0][x.hyp_start_idx:x.hyp_end_idx])\n",
    "for elem in inserted:\n",
    "    ins = ins + str(elem) + '\\n'\n",
    "with open('insert-COSER-4117-01', 'a', encoding='utf-8') as f:\n",
    "    f.write(ins)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-hf_sst]",
   "language": "python",
   "name": "conda-env-.conda-hf_sst-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
